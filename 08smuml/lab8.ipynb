{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8\n",
    "Cameron Matson\n",
    "\n",
    "Zihao Mao\n",
    "\n",
    "Yichen Duan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Music Reviews\n",
    "\n",
    "Our data set consists of every review from pitchfork.com since 1999, a total of 18,393 reviews of muisc albums.  Accompanied with the full lenght review, each album is rated on a scale from 0 to 10, 10 being the highest.  What we'd like to do is use a recurrent neural network to predict, from the text of the review itself, whether the reviewer gave the album a good score (8-10) or a bad one (0-7).  The reason for the unequal bin sizes is that this splits the data around 50/50 as we'll see.  For whatever reason 7 just seems to be average in the world of music reviews.\n",
    "\n",
    "One of the most common uses of sentiment analysis is to classify social media posts.  This type of information can then be used by marketing professionals as a sort of consumer survey.  This could potentially be used by music labels as well.  A music label might collect tweets, or facebook posts, or even articles on the web that mention an album that they recently released.  Then they could use our model (trained on the pitchfork reviews) to classify whether there is a consesus (or not) on how the album was recieved.\n",
    "\n",
    "In terms of metrics, since the business case is essentially to replace human observers of the data, and we probably won't have someone observing the output at the end (because then whats the point) accuracy is probably the best metric to use when evaluating our performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep\n",
    "\n",
    "The data is stored in two separate `csv` files, one that has the reviews in it, and one that has a bunch of meta data, including the score, in a different one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of reviews: 18393\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18393 entries, 0 to 18392\n",
      "Data columns (total 2 columns):\n",
      "reviewid    18393 non-null int64\n",
      "content     18383 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 287.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewid</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22703</td>\n",
       "      <td>“Trip-hop” eventually became a ’90s punchline,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22721</td>\n",
       "      <td>Eight years, five albums, and two EPs in, the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22659</td>\n",
       "      <td>Minneapolis’ Uranium Club seem to revel in bei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22661</td>\n",
       "      <td>Kleenex began with a crash. It transpired one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22725</td>\n",
       "      <td>It is impossible to consider a given release b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewid                                            content\n",
       "0     22703  “Trip-hop” eventually became a ’90s punchline,...\n",
       "1     22721  Eight years, five albums, and two EPs in, the ...\n",
       "2     22659  Minneapolis’ Uranium Club seem to revel in bei...\n",
       "3     22661  Kleenex began with a crash. It transpired one ...\n",
       "4     22725  It is impossible to consider a given release b..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "reviews = pd.read_csv('../data/reviews.csv', encoding='utf-8')\n",
    "print('number of reviews:',len(reviews))\n",
    "reviews.info()\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18393 entries, 0 to 18392\n",
      "Data columns (total 13 columns):\n",
      "reviewid          18393 non-null int64\n",
      "title             18391 non-null object\n",
      "artist            18391 non-null object\n",
      "url               18393 non-null object\n",
      "score             18393 non-null float64\n",
      "best_new_music    18393 non-null int64\n",
      "author            18393 non-null object\n",
      "author_type       14487 non-null object\n",
      "pub_date          18393 non-null object\n",
      "pub_weekday       18393 non-null int64\n",
      "pub_day           18393 non-null int64\n",
      "pub_month         18393 non-null int64\n",
      "pub_year          18393 non-null int64\n",
      "dtypes: float64(1), int64(6), object(6)\n",
      "memory usage: 1.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewid</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "      <th>best_new_music</th>\n",
       "      <th>author</th>\n",
       "      <th>author_type</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>pub_weekday</th>\n",
       "      <th>pub_day</th>\n",
       "      <th>pub_month</th>\n",
       "      <th>pub_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22703</td>\n",
       "      <td>mezzanine</td>\n",
       "      <td>massive attack</td>\n",
       "      <td>http://pitchfork.com/reviews/albums/22703-mezz...</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0</td>\n",
       "      <td>nate patrin</td>\n",
       "      <td>contributor</td>\n",
       "      <td>2017-01-08</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22721</td>\n",
       "      <td>prelapsarian</td>\n",
       "      <td>krallice</td>\n",
       "      <td>http://pitchfork.com/reviews/albums/22721-prel...</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0</td>\n",
       "      <td>zoe camp</td>\n",
       "      <td>contributor</td>\n",
       "      <td>2017-01-07</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22659</td>\n",
       "      <td>all of them naturals</td>\n",
       "      <td>uranium club</td>\n",
       "      <td>http://pitchfork.com/reviews/albums/22659-all-...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0</td>\n",
       "      <td>david glickman</td>\n",
       "      <td>contributor</td>\n",
       "      <td>2017-01-07</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22661</td>\n",
       "      <td>first songs</td>\n",
       "      <td>kleenex, liliput</td>\n",
       "      <td>http://pitchfork.com/reviews/albums/22661-firs...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>jenn pelly</td>\n",
       "      <td>associate reviews editor</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22725</td>\n",
       "      <td>new start</td>\n",
       "      <td>taso</td>\n",
       "      <td>http://pitchfork.com/reviews/albums/22725-new-...</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0</td>\n",
       "      <td>kevin lozano</td>\n",
       "      <td>tracks coordinator</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewid                 title            artist  \\\n",
       "0     22703             mezzanine    massive attack   \n",
       "1     22721          prelapsarian          krallice   \n",
       "2     22659  all of them naturals      uranium club   \n",
       "3     22661           first songs  kleenex, liliput   \n",
       "4     22725             new start              taso   \n",
       "\n",
       "                                                 url  score  best_new_music  \\\n",
       "0  http://pitchfork.com/reviews/albums/22703-mezz...    9.3               0   \n",
       "1  http://pitchfork.com/reviews/albums/22721-prel...    7.9               0   \n",
       "2  http://pitchfork.com/reviews/albums/22659-all-...    7.3               0   \n",
       "3  http://pitchfork.com/reviews/albums/22661-firs...    9.0               1   \n",
       "4  http://pitchfork.com/reviews/albums/22725-new-...    8.1               0   \n",
       "\n",
       "           author               author_type    pub_date  pub_weekday  pub_day  \\\n",
       "0     nate patrin               contributor  2017-01-08            6        8   \n",
       "1        zoe camp               contributor  2017-01-07            5        7   \n",
       "2  david glickman               contributor  2017-01-07            5        7   \n",
       "3      jenn pelly  associate reviews editor  2017-01-06            4        6   \n",
       "4    kevin lozano        tracks coordinator  2017-01-06            4        6   \n",
       "\n",
       "   pub_month  pub_year  \n",
       "0          1      2017  \n",
       "1          1      2017  \n",
       "2          1      2017  \n",
       "3          1      2017  \n",
       "4          1      2017  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = pd.read_csv('../data/meta.csv')\n",
    "meta.info()\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18393 entries, 0 to 18392\n",
      "Data columns (total 2 columns):\n",
      "reviewid    18393 non-null int64\n",
      "score       18393 non-null float64\n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 287.5 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewid</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22703</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22721</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22659</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22661</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22725</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewid  score\n",
       "0     22703    9.3\n",
       "1     22721    7.9\n",
       "2     22659    7.3\n",
       "3     22661    9.0\n",
       "4     22725    8.1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.drop(['title','artist', 'url', 'author', 'author_type', 'best_new_music', 'pub_date', 'pub_weekday', 'pub_day', 'pub_month', 'pub_year'], axis=1, inplace=True)\n",
    "meta.info()\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18391 entries, 0 to 18400\n",
      "Data columns (total 2 columns):\n",
      "content    18391 non-null object\n",
      "score      18391 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 431.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Trip-hop” eventually became a ’90s punchline,...</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eight years, five albums, and two EPs in, the ...</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Minneapolis’ Uranium Club seem to revel in bei...</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kleenex began with a crash. It transpired one ...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It is impossible to consider a given release b...</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  score\n",
       "0  “Trip-hop” eventually became a ’90s punchline,...    9.3\n",
       "1  Eight years, five albums, and two EPs in, the ...    7.9\n",
       "2  Minneapolis’ Uranium Club seem to revel in bei...    7.3\n",
       "3  Kleenex began with a crash. It transpired one ...    9.0\n",
       "4  It is impossible to consider a given release b...    8.1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(reviews, meta, on='reviewid')\n",
    "df.drop('reviewid', axis=1, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets group the score column so that we have a categorical target variable.  And then take a look at the distribution of the classes we set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15f6f51d390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "THRESH = 7\n",
    "labels = np.arange(NUM_CLASSES)\n",
    "df['target'] = pd.cut(df.score, [-1, 7.0, 10.0], labels=labels)\n",
    "#df['target'] = pd.cut(df.score, NUM_CLASSES, labels=labels)\n",
    "df.head()\n",
    "\n",
    "plt.hist(df.target, bins=NUM_CLASSES)\n",
    "plt.xticks(labels)\n",
    "plt.yticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've decided to do here is make it a binary classification problem, but to set the threshold at a review score of 7.  This is because most albums get pretty favorable reviews.  You can see this in the distribution, as even when the top bin accounts for more than half of all reviews.  When we do our cross-validation we'll use stratified splits so it shouldn't be that important, but we'll cover it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#finally lets reduce the number of articles we're working with\n",
    "NUM_SAMPLES = 5000\n",
    "df = df.sample(n=NUM_SAMPLES, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Waco Brothers should be awful. There shouldn\\'t be any way possible on\\n    God\\'s green earth that a British art-punk with anarchosyndicalist leanings\\n    could put together a band that includes an acknowledged ex-member of KMFDM\\n    on pedal steel guitar and make credible-sounding circa-1960 roots country.\\n    Worse, this lot of expats do it a lot better than most Americans of the same\\n    ilk.\\n    \\n    Admit it. It rankles a little bit when someone from exotic climes with his or\\n    her own cultural legacy waiting to be exploited comes along and exploits ours\\n    better than we do. By all rights, Jon Langford\\'s roots project ought to be a\\n    collection of rockin\\' madrigals and ballads about flowers. We\\'ve got a brace\\n    of alt-country upstarts who can defame our glorious traditions just fine,\\n    thanks. Where does this pack of goofy bastards get off thinking they can\\n    one-up our national heritage?\\n    \\n    But they do, damn it. Jon Langford has always utilized his extracurricular\\n    time with the Wacos to exorcise the C&W; demons that have lurked in his work\\n    with the Mekons since the 1980s. Where great Mekons albums integrate a vaguely\\n    folkish, space cowboy vibe, the Waco Brothers sound like real-deal ten-gallon\\n    hatters with a grouchy streak and a subscription to The Nation. There\\n    are no body-positive-feminist allegories about pirates to be found here. Sure,\\n    there\\'s a William Blake nod on occasion, but mostly, Langford and\\n    co-singer/songwriter Dean Schlabowske invoke the spirit of the common man,\\n    crank up the hollow-body guitars and rock their wary ways.\\n    \\n    Hoping for artistic growth is completely off the subject when talking about\\n    the Waco Brothers. After all, the band\\'s an acknowledged nostalgia act. All\\n    the same, despite its bellicose title, Electric Waco Chair finds our\\n    boys in a relatively contemplative mood: the songs center generally on getting\\n    older; the tempos aren\\'t nearly as frantic as they\\'ve been in the past; and\\n    the jokes here are at least a little bit subtler. The Wacos still want to be\\n    the party band in the honky-tonk at the end of the world, but they\\'ve come to\\n    realize they need a couple slow jams for couples to dance to.\\nChair, eager to please as other Waco offerings, includes a brace of\\n    crowdpleasers. The air of scabrousness that hovers over most Mekons projects\\n    isn\\'t anywhere to be found here, replaced instead with a rummy goodheartedness\\n    in short supply among independent-label socialists these days. Langford seems\\n    unnecessarily into channeling Billy Bragg at times-- particularly on the\\n    egregious \"Walking on Hell\\'s Roof Looking at the Flowers\"-- and some of the\\n    arrangements are a little shticky, like when the Brothers attempt a\\n    Spanish-flavored feel on \"Cornered.\"  But nit-picking individual moments here\\n    is pointless spoil-sportsmanship. It\\'s like going to a great rent party and\\n    complaining that the beer sucks.\\n    \\n    The Waco Brothers aren\\'t about flawless, detailed songcraft. They\\'re about\\n    lager-soaked good times with just enough anxiety and doubt to make the trip\\n    worth taking.  And, in that respect, Electric Waco Chair never\\n    disappoints. Though these fellas don\\'t take the country idiom as seriously as\\n    your favorite American punk-countryist might, they sure have a great time\\n    doing their work.\\n    \\n    If anything, their irreverence toward country music is what separates the\\n    Waco Brothers from the herd. Where American revivalists are frustratingly\\n    serious about their music, treating country like holy writ, the Waco Brothers\\n    don\\'t have the same stake in it. Because it\\'s not their history, they can\\n    treat it as a contemporary event and return it to the go-for-broke fun spirit\\n    behind all that worship old-time tunage gets subjected to. Besides, who wants\\n    to listen to madrigals all day, anyhow?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is an array of the reviews\n",
    "texts = df.content.values\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use Keras's tokenizer module and pad_sequences to convert the texts into integer sequences of all the same length which makes the computation much easier and quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (5000, 500)\n",
      "Shape of label tensor: (5000, 2)\n",
      "749\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS = 750\n",
    "MAX_ART_LEN = 500\n",
    "\n",
    "tkn = Tokenizer(num_words=NUM_WORDS)\n",
    "tkn.fit_on_texts(texts)\n",
    "\n",
    "sequences = tkn.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "\n",
    "y = np.asanyarray(df.target)\n",
    "y_ohe = keras.utils.to_categorical(y)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', y_ohe.shape)\n",
    "print(np.max(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN stuff\n",
    "\n",
    "Okay now our data is ready to use for in the rnn.\n",
    "\n",
    "We'll design two networks one using a LSTM cell and one using a GRU.  For each of them we'll try a few different hyper parameters first on each of them to see if we can find the best architecture.  After that we'll do a cross validation loop of five iterations on an 80/20 train/test split to evaluate the general performance\n",
    "\n",
    "The parameters we'll investigate are dropout/recurrent_dropout (we'll move these together as one variable) and the output dimension of the RNN cell.\n",
    "\n",
    "Both the LSTM and GRU architectures will have the same overall structure\n",
    "- embedding (which will be shared between the two architectures)\n",
    "- the rnn cell\n",
    "- 2 dense layers (one with a hidden number of neurons and the final with the number of classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "dropout_rates = [0.0, 0.2, 0.6]\n",
    "latent_dims = [20, 30, 50]\n",
    "\n",
    "# for testing\n",
    "#dropout_rates = [0.2]\n",
    "#latent_dims = [50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "EMBED_SIZE = 50\n",
    "\n",
    "archs = ['LSTM', 'GRU']\n",
    "#for testing\n",
    "#archs = ['LSTM']\n",
    "\n",
    "dropout = dropout_rates[0]\n",
    "latent_dim = latent_dims[0]\n",
    "\n",
    "input_holder = Input(shape=(MAX_ART_LEN, ), name='in')\n",
    "shared_embed = Embedding(NUM_WORDS, \n",
    "                         EMBED_SIZE, \n",
    "                         input_length=MAX_ART_LEN, name='embed')(input_holder)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****** LSTM : 0.0 , 20 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 20)                5680      \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                1050      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 44,332\n",
      "Trainable params: 44,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "****** LSTM : 0.0 , 30 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 30)                9720      \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                1550      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 48,872\n",
      "Trainable params: 48,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "****** LSTM : 0.0 , 50 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 60,352\n",
      "Trainable params: 60,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "****** LSTM : 0.2 , 20 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 20)                5680      \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                1050      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 44,332\n",
      "Trainable params: 44,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "****** LSTM : 0.2 , 30 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 30)                9720      \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                1550      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 48,872\n",
      "Trainable params: 48,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "****** LSTM : 0.2 , 50 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 60,352\n",
      "Trainable params: 60,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "****** LSTM : 0.6 , 20 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 20)                5680      \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                1050      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 44,332\n",
      "Trainable params: 44,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "****** LSTM : 0.6 , 30 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 30)                9720      \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                1550      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 48,872\n",
      "Trainable params: 48,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****** LSTM : 0.6 , 50 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 60,352\n",
      "Trainable params: 60,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "****** GRU : 0.0 , 20 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "GRU (GRU)                    (None, 20)                4260      \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                1050      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 42,912\n",
      "Trainable params: 42,912\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "****** GRU : 0.0 , 30 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "GRU (GRU)                    (None, 30)                7290      \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                1550      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 46,442\n",
      "Trainable params: 46,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "****** GRU : 0.0 , 50 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "GRU (GRU)                    (None, 50)                15150     \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 55,302\n",
      "Trainable params: 55,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "****** GRU : 0.2 , 20 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "GRU (GRU)                    (None, 20)                4260      \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                1050      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 42,912\n",
      "Trainable params: 42,912\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "****** GRU : 0.2 , 30 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "GRU (GRU)                    (None, 30)                7290      \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                1550      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 46,442\n",
      "Trainable params: 46,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "****** GRU : 0.2 , 50 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "GRU (GRU)                    (None, 50)                15150     \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 55,302\n",
      "Trainable params: 55,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "****** GRU : 0.6 , 20 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "GRU (GRU)                    (None, 20)                4260      \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                1050      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 42,912\n",
      "Trainable params: 42,912\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****** GRU : 0.6 , 30 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "GRU (GRU)                    (None, 30)                7290      \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                1550      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 46,442\n",
      "Trainable params: 46,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "****** GRU : 0.6 , 50 ********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 500, 50)           37500     \n",
      "_________________________________________________________________\n",
      "GRU (GRU)                    (None, 50)                15150     \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 55,302\n",
      "Trainable params: 55,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "rnns = []\n",
    "for arch in archs:\n",
    "    for dropout in dropout_rates:\n",
    "        for latent_dim in latent_dims:\n",
    "            func = eval(arch)\n",
    "            rnn_cell = func(latent_dim, dropout=dropout, recurrent_dropout=dropout, name=arch)(shared_embed)\n",
    "            hidden = Dense(50, activation='relu', name='hidden')(rnn_cell)\n",
    "            out = Dense(NUM_CLASSES, activation='softmax', name='out')(hidden)\n",
    "\n",
    "            rnn=Model(inputs=input_holder, outputs=out)\n",
    "            rnn.compile(loss='binary_crossentropy', \n",
    "                          optimizer='rmsprop', \n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            print('\\n\\n******', arch, ':', dropout, ',', latent_dim, '********')\n",
    "            print(rnn.summary())\n",
    "            rnns.append((rnn, (arch, dropout, latent_dim)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rnns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_ohe, test_size=0.35, stratify=y_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "fitting  ('LSTM', 0.0, 20)\n",
      "Train on 3250 samples, validate on 1750 samples\n",
      "Epoch 1/3\n",
      "3250/3250 [==============================] - 33s - loss: 0.6655 - acc: 0.5720 - val_loss: 0.6703 - val_acc: 0.5977\n",
      "Epoch 2/3\n",
      "3250/3250 [==============================] - 33s - loss: 0.6257 - acc: 0.6631 - val_loss: 0.6725 - val_acc: 0.6080\n",
      "Epoch 3/3\n",
      "3250/3250 [==============================] - 33s - loss: 0.5990 - acc: 0.6782 - val_loss: 0.6826 - val_acc: 0.6080\n",
      "\n",
      "\n",
      "fitting  ('LSTM', 0.0, 30)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m       \u001b[0mxla_compile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_XlaCompile\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1666\u001b[0m       raise ValueError(\"No attr named '\" + name + \"' in \" +\n\u001b[1;32m-> 1667\u001b[1;33m                        str(self._node_def))\n\u001b[0m\u001b[0;32m   1668\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_node_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No attr named '_XlaCompile' in name: \"LSTM_10/while/Tanh\"\nop: \"Tanh\"\ninput: \"LSTM_10/while/add_4\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-a186e7642d2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrnn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrnns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\nfitting '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mrnn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1573\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1574\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1575\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1576\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    958\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[0;32m    959\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                         loss=self.total_loss)\n\u001b[0m\u001b[0;32m    961\u001b[0m                 \u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtraining_updates\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                 \u001b[1;31m# Gets loss and metrics. Updates weights at each call.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_updates_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m         \u001b[0maccumulators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccumulators\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'clipnorm'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclipnorm\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[1;34m(loss, variables)\u001b[0m\n\u001b[0;32m   2308\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2309\u001b[0m     \"\"\"\n\u001b[1;32m-> 2310\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\u001b[0m\n\u001b[0;32m    538\u001b[0m                 \u001b[1;31m# functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m                 in_grads = _MaybeCompile(\n\u001b[1;32m--> 540\u001b[1;33m                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[0;32m    541\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m                 \u001b[1;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    344\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_XlaScope\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Exit early\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    538\u001b[0m                 \u001b[1;31m# functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m                 in_grads = _MaybeCompile(\n\u001b[1;32m--> 540\u001b[1;33m                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[0;32m    541\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m                 \u001b[1;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_TanhGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tanh_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m_tanh_grad\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   2575\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2576\u001b[0m   \"\"\"\n\u001b[1;32m-> 2577\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TanhGrad\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2578\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    768\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2506\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2507\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2508\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2509\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2510\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1871\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1873\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1874\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1875\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[0;32m    611\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    666\u001b[0m   \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m       output = pywrap_tensorflow.RunCppShapeInference(\n\u001b[0;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m   \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewStatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;32myield\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as mt\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for rnn in rnns:\n",
    "    print('\\n\\nfitting ', rnn[1])\n",
    "    rnn[0].fit(X_train, y_train, batch_size=100, epochs=3, validation_data=[X_test, y_test])\n",
    "    \n",
    "    yhat = np.round(rnn[0].predict(X_test))\n",
    "    acc = mt.accuracy_score(yhat, y_test)\n",
    "    accuracies.append((acc, rnn[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"You're the best. AROUND!\"\n",
    "\n",
    "Let's pick the best one (in terms of overall accuracy) and visualize how it did and how its perfromance might generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model we made was... ('LSTM', 0.0, 20)\n",
      "...with a performance of  0.608\n"
     ]
    }
   ],
   "source": [
    "BEST_IDX = np.argmax([a for (a, n) in accuracies])\n",
    "\n",
    "print('The best model we made was...', accuracies[BEST_IDX][1])\n",
    "print('...with a performance of ', accuracies[BEST_IDX][0])\n",
    "\n",
    "best_model = rnns[BEST_IDX][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "def confusion_matrix_plot(rnn, X_test, y_test):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    yhat2 = np.round(rnn.predict(X_test))\n",
    "    yhat = np.argmax(rnn.predict(X_test), axis=1)\n",
    "    rec_rnn = mt.accuracy_score(y_test,yhat2)\n",
    "    plt.subplot(1,3,1)\n",
    "    cm = mt.confusion_matrix(np.argmax(y_test, axis=1),yhat)\n",
    "    cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f')\n",
    "    plt.title('best')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAE/CAYAAACXe3JQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwqrhCgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15f06f55f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix_plot(best_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation\n",
    "\n",
    "Now we'll pick the best model and do cross validation to see how the performance of it might generalize.  We'll use a stratified 5-fold cv loop.  We'll use K-fold because we've limited our dataset size to speed things up a bit and that way we can train on all of the data that we do have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CV  1\n",
      "Train on 3999 samples, validate on 1001 samples\n",
      "Epoch 1/3\n",
      "2500/3999 [=================>............] - ETA: 12s - loss: 0.6210 - acc: 0.6560"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "NUM_SPLITS = 5\n",
    "skf = StratifiedKFold(NUM_SPLITS, shuffle=False)\n",
    "\n",
    "cv_acc = []\n",
    "\n",
    "for i, (train, test) in enumerate(skf.split(X, y)):\n",
    "    X_train = X[train]\n",
    "    X_test = X[test]\n",
    "    y_train = keras.utils.to_categorical(y[train])\n",
    "    y_test = keras.utils.to_categorical(y[test])\n",
    "    \n",
    "    print('\\n\\nCV ', i+1)\n",
    "    best_model.fit(X_train, y_train, batch_size=100, epochs=3, validation_data=[X_test, y_test])\n",
    "    \n",
    "    yhat = np.round(rnn[0].predict(X_test))\n",
    "    acc = mt.accuracy_score(yhat, y_test)\n",
    "    cv_acc.append(acc)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:', np.mean(cv_acc), '+-', np.std(cv_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Visualization\n",
    "\n",
    "Lets take a look at how our embedding is doing.  Ideally what we'd like to see is that words with similar meanings are relatively close together in the embedding space while opposite words are far apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grabe the weights of the embedding\n",
    "embed_layer = best_model.get_layer('embed')  # note that since this was shared,\n",
    "                                             # it doesn't really matter which model we pick it from\n",
    "weights = embed_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below takes four words as inputs, consisting of two pairs of \"opposites.\"  The two pairs themselves should have similar connotations though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_weights(w1, w2, w3, w4):\n",
    "    # get the index of each word\n",
    "    p1 = tkn.word_index[w1]\n",
    "    p2 = tkn.word_index[w2]\n",
    "    p3 = tkn.word_index[w3]\n",
    "    p4 = tkn.word_index[w4]\n",
    "    \n",
    "    # embedding vectors\n",
    "    e1 = weights[p1]\n",
    "    e2 = weights[p2]\n",
    "    e3 = weights[p3]\n",
    "    e4 = weights[p4]\n",
    "\n",
    "    # we'll project into the space defined by the greatest distance between each pair\n",
    "    distance1 = np.abs(e1 - e2)\n",
    "    distance2 = np.abs(e3 - e4)\n",
    "    \n",
    "    d1 = np.argmax(distance1)\n",
    "    d2 = np.argmax(distance2)\n",
    "\n",
    "    # projections of each vector into the plane\n",
    "    e1_2d = (e1[d1], e1[d2])\n",
    "    e2_2d = (e2[d1], e2[d2])\n",
    "    e3_2d = (e3[d1], e3[d2])\n",
    "    e4_2d = (e4[d1], e4[d2])\n",
    "\n",
    "    # plot\n",
    "    plt.scatter([e1_2d[0], e2_2d[0], e3_2d[0], e4_2d[0]],[e1_2d[1], e2_2d[1], e3_2d[1], e4_2d[1]])\n",
    "    plt.annotate(w1, e1_2d)\n",
    "    plt.annotate(w2, e2_2d)\n",
    "    plt.annotate(w3, e3_2d)\n",
    "    plt.annotate(w4, e4_2d)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 'loud'\n",
    "w2 = 'quiet'\n",
    "w3 = 'hard'\n",
    "w4 = 'soft'\n",
    "\n",
    "visualize_weights(w1, w2, w3, w4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlenv]",
   "language": "python",
   "name": "conda-env-mlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
