{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1102</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>279</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1373</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1392</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>591</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
       "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
       "1   49        No  Travel_Frequently        279  Research & Development   \n",
       "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
       "3   33        No  Travel_Frequently       1392  Research & Development   \n",
       "4   27        No      Travel_Rarely        591  Research & Development   \n",
       "\n",
       "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
       "0                 1          2  Life Sciences              1               1   \n",
       "1                 8          1  Life Sciences              1               2   \n",
       "2                 2          2          Other              1               4   \n",
       "3                 3          4  Life Sciences              1               5   \n",
       "4                 2          1        Medical              1               7   \n",
       "\n",
       "           ...           RelationshipSatisfaction StandardHours  \\\n",
       "0          ...                                  1            80   \n",
       "1          ...                                  4            80   \n",
       "2          ...                                  2            80   \n",
       "3          ...                                  3            80   \n",
       "4          ...                                  4            80   \n",
       "\n",
       "   StockOptionLevel  TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  \\\n",
       "0                 0                  8                      0               1   \n",
       "1                 1                 10                      3               3   \n",
       "2                 0                  7                      3               3   \n",
       "3                 0                  8                      3               3   \n",
       "4                 1                  6                      3               3   \n",
       "\n",
       "   YearsAtCompany YearsInCurrentRole  YearsSinceLastPromotion  \\\n",
       "0               6                  4                        0   \n",
       "1              10                  7                        1   \n",
       "2               0                  0                        0   \n",
       "3               8                  7                        3   \n",
       "4               2                  2                        2   \n",
       "\n",
       "   YearsWithCurrManager  \n",
       "0                     5  \n",
       "1                     7  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../data/'\n",
    "df = pd.read_csv(os.path.join(data_path, 'WA_Fn-UseC_-HR-Employee-Attrition.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 35 features in total in the dataset about. \n",
    "Let's focus on a few of them:\n",
    "- Age\n",
    "- Attrition\n",
    "- Department\n",
    "- DistanceFromHome\n",
    "- Education\n",
    "- EnvironmentSatisfaction\n",
    "- Gender\n",
    "- JobSatisfaction\n",
    "- MaritalStatus\n",
    "- MonthlyIncome\n",
    "- OverTime\n",
    "- PerformanceRating\n",
    "- RelationshipSatisfaction\n",
    "- TotalWorkingYears\n",
    "- YearsAtCompany\n",
    "\n",
    "In this lab, we will use attrition as our label, to try to predict the attrition status accroding to other attributes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470 entries, 0 to 1469\n",
      "Data columns (total 15 columns):\n",
      "Age                         1470 non-null int64\n",
      "Attrition                   1470 non-null object\n",
      "Department                  1470 non-null object\n",
      "DistanceFromHome            1470 non-null int64\n",
      "Education                   1470 non-null int64\n",
      "EnvironmentSatisfaction     1470 non-null int64\n",
      "Gender                      1470 non-null object\n",
      "JobSatisfaction             1470 non-null int64\n",
      "MaritalStatus               1470 non-null object\n",
      "MonthlyIncome               1470 non-null int64\n",
      "OverTime                    1470 non-null object\n",
      "PerformanceRating           1470 non-null int64\n",
      "RelationshipSatisfaction    1470 non-null int64\n",
      "TotalWorkingYears           1470 non-null int64\n",
      "YearsAtCompany              1470 non-null int64\n",
      "dtypes: int64(10), object(5)\n",
      "memory usage: 172.3+ KB\n"
     ]
    }
   ],
   "source": [
    "to_keep = {'Age', 'Attrition', 'Department','DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'Gender', 'JobSatisfaction', 'MaritalStatus',\n",
    "           'MonthlyIncome', 'OverTime', 'PerformanceRating', 'RelationshipSatisfaction','TotalWorkingYears','YearsAtCompany'}\n",
    "to_drop = set(df.columns)-to_keep\n",
    "df.drop(to_drop, axis=1, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good that we don't have any null value. Let's one hot encode the Attrition, Department, Gender, MaritalStatus and Overtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_convert = ['Education','EnvironmentSatisfaction','JobSatisfaction',\n",
    "            'PerformanceRating','RelationshipSatisfaction']\n",
    "for col in to_convert:\n",
    "    df[col] = df[col].astype(np.str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "to_encode = {'Attrition', 'Department','Gender','MaritalStatus','OverTime','Education','EnvironmentSatisfaction','JobSatisfaction',\n",
    "            'PerformanceRating','RelationshipSatisfaction'}\n",
    "encoders = dict()\n",
    "\n",
    "for col in to_encode:\n",
    "    if col==\"attrition\":\n",
    "        tmp = LabelEncoder()\n",
    "        df[col] = tmp.fit_transform(df[col])\n",
    "    else:\n",
    "        encoders[col] = LabelEncoder()\n",
    "        df[col+'_int'] = encoders[col].fit_transform(df[col])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's scale the numeric features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_features =list(to_encode)\n",
    "categorical_features = [x+'_int' for x in categorical_features]\n",
    "numerics = set(df.columns) - to_encode\n",
    "numerics = list(numerics - set(categorical_features))\n",
    "\n",
    "for atr in numerics:\n",
    "    df[atr] = df[atr].astype(np.float)    \n",
    "    ss = StandardScaler()\n",
    "    df[atr] = ss.fit_transform(df[atr].values.reshape(-1, 1))\n",
    "    \n",
    "feature_columns = categorical_features + numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DistanceFromHome',\n",
       " 'TotalWorkingYears',\n",
       " 'Age',\n",
       " 'YearsAtCompany',\n",
       " 'MonthlyIncome']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470 entries, 0 to 1469\n",
      "Data columns (total 25 columns):\n",
      "Age                             1470 non-null float64\n",
      "Attrition                       1470 non-null object\n",
      "Department                      1470 non-null object\n",
      "DistanceFromHome                1470 non-null float64\n",
      "Education                       1470 non-null object\n",
      "EnvironmentSatisfaction         1470 non-null object\n",
      "Gender                          1470 non-null object\n",
      "JobSatisfaction                 1470 non-null object\n",
      "MaritalStatus                   1470 non-null object\n",
      "MonthlyIncome                   1470 non-null float64\n",
      "OverTime                        1470 non-null object\n",
      "PerformanceRating               1470 non-null object\n",
      "RelationshipSatisfaction        1470 non-null object\n",
      "TotalWorkingYears               1470 non-null float64\n",
      "YearsAtCompany                  1470 non-null float64\n",
      "Gender_int                      1470 non-null int64\n",
      "Education_int                   1470 non-null int64\n",
      "EnvironmentSatisfaction_int     1470 non-null int64\n",
      "Department_int                  1470 non-null int64\n",
      "JobSatisfaction_int             1470 non-null int64\n",
      "RelationshipSatisfaction_int    1470 non-null int64\n",
      "Attrition_int                   1470 non-null int64\n",
      "OverTime_int                    1470 non-null int64\n",
      "PerformanceRating_int           1470 non-null int64\n",
      "MaritalStatus_int               1470 non-null int64\n",
      "dtypes: float64(5), int64(10), object(10)\n",
      "memory usage: 287.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "# this uses the Keras Wrapper to make the model usable by sk-learn\n",
    "\n",
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(num_neurons=12, input_dim=8):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # num_neurons is a list of the number nuerons at each layer\n",
    "    for layer, num in enumerate(num_neurons):\n",
    "        if layer == 0:\n",
    "            model.add(Dense(num, input_dim=input_dim, activation='relu'))\n",
    "        else:\n",
    "            model.add(Dense(units=num, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can creat models in that can be used by SKLearn, lets GridSearch over the parameters we identified.  Note that the number of neurons and number of layers is combined into one parameter called `num_neurons` which is a list of the number of output nerouns at each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (1323, 15) test (147, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.layers import Embedding, Flatten, Merge, concatenate\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# stratified 90/10 train/test split`\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, stratify=df.Attrition)\n",
    "\n",
    "X_train = ss.fit_transform(df_train[feature_columns].values).astype(np.float32)\n",
    "X_test = ss.fit_transform(df_test[feature_columns].values).astype(np.float32)\n",
    "\n",
    "y_train = df_train['Attrition_int'].values.astype(np.int)\n",
    "y_test = df_test['Attrition_int'].values.astype(np.int)\n",
    "\n",
    "print('train', X_train.shape, 'test', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_columns = [['Gender','MaritalStatus'],\n",
    "                    ['Education', 'JobSatisfaction'],['Department','PerformanceRating'],\n",
    "                    ['Education', 'JobSatisfaction','RelationshipSatisfaction'],['Department','OverTime'],\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "X_train_ohe = ohe.fit_transform(df_train[categorical_features].values)\n",
    "X_test_ohe = ohe.transform(df_test[categorical_features].values)\n",
    "\n",
    "\n",
    "X_train_num =  df_train[numerics].values\n",
    "X_test_num = df_test[numerics].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ce87d8e4e0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to create separate sequential models for each embedding\n",
    "embed_branches = []\n",
    "X_ints_train = []\n",
    "X_ints_test = []\n",
    "all_inputs = []\n",
    "all_branch_outputs = []\n",
    "\n",
    "\n",
    "# reset this input branch\n",
    "all_branch_outputs = []\n",
    "# add in the embeddings\n",
    "for col in categorical_features:\n",
    "    # encode as ints for the embedding\n",
    "    X_ints_train.append( df_train[col].values )\n",
    "    X_ints_test.append( df_test[col].values )\n",
    "    \n",
    "    # get the number of categories\n",
    "    N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "    \n",
    "    # create embedding branch from the number of categories\n",
    "    inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "    all_inputs.append(inputs)\n",
    "    x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "    x = Flatten()(x)\n",
    "    all_branch_outputs.append(x)\n",
    "    \n",
    "# also get a dense branch of the numeric features\n",
    "all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "all_branch_outputs.append( x )\n",
    "\n",
    "# merge the branches together\n",
    "deep_branch = concatenate(all_branch_outputs)\n",
    "    \n",
    "final_branch = Dense(units=1,activation='sigmoid')(deep_branch)\n",
    "\n",
    "model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "model.compile(optimizer='adagrad',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_ints_train+ [X_train_num],\n",
    "        y_train, epochs=10, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model(inputs=model.inputs, outputs=model.get_layer(index=-2).output)\n",
    "\n",
    "X_embed = model2.predict(X_ints_train+ [X_train_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10] \n",
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10], score=0.987012987012987, total=   2.2s\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10], score=0.9838709677419355, total=   2.7s\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    5.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10], score=0.9459459459459459, total=   2.3s\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10, 20] \n",
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10, 20], score=1.0, total=   3.9s\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10, 20] \n",
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10, 20], score=1.0, total=   2.4s\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10, 20] \n",
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10, 20], score=1.0, total=   2.6s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10] ..............\n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10], score=0.24675324675324675, total=   2.6s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10] ..............\n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10], score=0.0, total=   2.5s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10] ..............\n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10], score=0.0, total=   4.2s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10, 20] ..........\n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10, 20], score=0.987012987012987, total=   3.1s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10, 20] ..........\n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10, 20], score=0.5645161290322581, total=   4.0s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10, 20] ..........\n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10, 20], score=0.8918918918918919, total=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:   37.2s finished\n"
     ]
    }
   ],
   "source": [
    "# this will move inside nested CV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "num_neurons = [[5, 10], [5, 10, 20]]\n",
    "class_weight=class_weight = [{0:x, 1:1-x} for x in np.linspace(0.1, 0.5, 2)]\n",
    "param_grid = dict(num_neurons=num_neurons,\n",
    "                  class_weight=class_weight)\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, input_dim=X_embed.shape[-1], epochs=10, verbose=0)\n",
    "g = GridSearchCV(estimator=model, param_grid=param_grid, verbose=3, scoring='recall')\n",
    "r = g.fit(X_embed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_193 (Dense)            (None, 5)                 175       \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 10)                60        \n",
      "_________________________________________________________________\n",
      "dense_195 (Dense)            (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 476\n",
      "Trainable params: 476\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "{'input_dim': 34, 'epochs': 10, 'verbose': 0, 'class_weight': {0: 0.10000000000000001, 1: 0.90000000000000002}, 'num_neurons': [5, 10, 20], 'build_fn': <function create_model at 0x000002CE87521D90>}\n"
     ]
    }
   ],
   "source": [
    "best_deep = r.best_estimator_.model\n",
    "print(best_deep.summary())\n",
    "best_deep_params = r.best_estimator_.get_params()\n",
    "print(best_deep_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1323/1323 [==============================] - 1s - loss: 0.1246 - acc: 0.3583     \n",
      "Epoch 2/10\n",
      "1323/1323 [==============================] - 0s - loss: 0.0725 - acc: 0.8670     \n",
      "Epoch 3/10\n",
      "1323/1323 [==============================] - 0s - loss: 0.0362 - acc: 0.9902     \n",
      "Epoch 4/10\n",
      "1323/1323 [==============================] - 0s - loss: 0.0194 - acc: 0.9985     \n",
      "Epoch 5/10\n",
      "1323/1323 [==============================] - 0s - loss: 0.0118 - acc: 1.0000     \n",
      "Epoch 6/10\n",
      "1323/1323 [==============================] - 0s - loss: 0.0078 - acc: 1.0000     \n",
      "Epoch 7/10\n",
      "1323/1323 [==============================] - 0s - loss: 0.0055 - acc: 1.0000     \n",
      "Epoch 8/10\n",
      "1323/1323 [==============================] - 0s - loss: 0.0041 - acc: 1.0000     \n",
      "Epoch 9/10\n",
      "1323/1323 [==============================] - 0s - loss: 0.0032 - acc: 1.0000     \n",
      "Epoch 10/10\n",
      "1323/1323 [==============================] - 0s - loss: 0.0026 - acc: 1.0000     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ce8a429fd0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# we need to create separate sequential models for each embedding\n",
    "embed_branches = []\n",
    "X_ints_train = []\n",
    "X_ints_test = []\n",
    "all_inputs = []\n",
    "all_branch_outputs = []\n",
    "\n",
    "for cols in cross_columns:\n",
    "    # encode crossed columns as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "    \n",
    "    # create crossed labels\n",
    "    # needs to be commented better, Eric!\n",
    "    X_crossed_train = df_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "    X_crossed_test = df_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "    \n",
    "    enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "    X_crossed_train = enc.transform(X_crossed_train)\n",
    "    X_crossed_test = enc.transform(X_crossed_test)\n",
    "    X_ints_train.append( X_crossed_train )\n",
    "    X_ints_test.append( X_crossed_test )\n",
    "    \n",
    "    # get the number of categories\n",
    "    N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "    \n",
    "    # create embedding branch from the number of categories\n",
    "    inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "    all_inputs.append(inputs)\n",
    "    x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "    x = Flatten()(x)\n",
    "    all_branch_outputs.append(x)\n",
    "    \n",
    "# merge the branches together\n",
    "wide_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "# reset this input branch\n",
    "all_branch_outputs = []\n",
    "# add in the embeddings\n",
    "for col in categorical_features:\n",
    "    # encode as ints for the embedding\n",
    "    X_ints_train.append( df_train[col].values )\n",
    "    X_ints_test.append( df_test[col].values )\n",
    "    \n",
    "    # get the number of categories\n",
    "    N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "    \n",
    "    # create embedding branch from the number of categories\n",
    "    inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "    all_inputs.append(inputs)\n",
    "    x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "    x = Flatten()(x)\n",
    "    all_branch_outputs.append(x)\n",
    "    \n",
    "# also get a dense branch of the numeric features\n",
    "all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "all_branch_outputs.append( x )\n",
    "\n",
    "# merge the branches together\n",
    "deep_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "# here is where we'll use the result of the GridSearch\n",
    "for layer in best_deep.layers[:-1]:\n",
    "    deep_branch = layer(deep_branch)\n",
    "    \n",
    "final_branch = concatenate([wide_branch, deep_branch])\n",
    "final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "\n",
    "model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "model.compile(optimizer='adagrad',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_ints_train+ [X_train_num],\n",
    "        y_train, epochs=10, batch_size=32, verbose=1, class_weight=best_deep_params['class_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[123   0]\n",
      " [  0  24]] 1.0\n"
     ]
    }
   ],
   "source": [
    "yhat = np.round(model.predict(X_ints_test + [X_test_num]))\n",
    "print(mt.confusion_matrix(y_test,yhat),mt.recall_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training embedings\n",
      "Epoch 1/10\n",
      "1175/1175 [==============================] - 2s - loss: 0.1654 - acc: 0.8323     \n",
      "Epoch 2/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1404 - acc: 0.8391     \n",
      "Epoch 3/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1326 - acc: 0.8391     \n",
      "Epoch 4/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1272 - acc: 0.8391     \n",
      "Epoch 5/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1225 - acc: 0.8391     \n",
      "Epoch 6/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1183 - acc: 0.8391     \n",
      "Epoch 7/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1143 - acc: 0.8391     \n",
      "Epoch 8/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1106 - acc: 0.8400     \n",
      "Epoch 9/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1071 - acc: 0.8409     \n",
      "Epoch 10/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1038 - acc: 0.8477     \n",
      "Grid Searching the layers of the Deep\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting the whole thing\n",
      "Epoch 1/10\n",
      "1175/1175 [==============================] - 2s - loss: 0.0733 - acc: 0.8417     \n",
      "Epoch 2/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.0190 - acc: 1.0000     \n",
      "Epoch 3/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.0088 - acc: 1.0000     \n",
      "Epoch 4/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.0054 - acc: 1.0000     \n",
      "Epoch 5/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.0037 - acc: 1.0000     \n",
      "Epoch 6/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.0028 - acc: 1.0000     \n",
      "Epoch 7/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.0022 - acc: 1.0000     \n",
      "Epoch 8/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.0018 - acc: 1.0000     \n",
      "Epoch 9/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.0015 - acc: 1.0000     \n",
      "Epoch 10/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "1.0\n",
      "training embedings\n",
      "Epoch 1/10\n",
      "1175/1175 [==============================] - 2s - loss: 0.1585 - acc: 0.8349     \n",
      "Epoch 2/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1325 - acc: 0.8391     \n",
      "Epoch 3/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1257 - acc: 0.8391     \n",
      "Epoch 4/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1211 - acc: 0.8391     \n",
      "Epoch 5/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1171 - acc: 0.8391     \n",
      "Epoch 6/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1134 - acc: 0.8391     \n",
      "Epoch 7/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1100 - acc: 0.8409     \n",
      "Epoch 8/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1067 - acc: 0.8417     \n",
      "Epoch 9/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1036 - acc: 0.8443     \n",
      "Epoch 10/10\n",
      "1175/1175 [==============================] - 0s - loss: 0.1006 - acc: 0.8536     \n",
      "Grid Searching the layers of the Deep\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "outer_loop = StratifiedKFold(n_splits=5)\n",
    "\n",
    "X = df.drop('Attrition', axis=1).values\n",
    "y = df.Attrition.values\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_idx, test_idx in outer_loop.split(X, y):\n",
    "    \n",
    "    # split data\n",
    "    df_train = pd.DataFrame(X[train_idx], columns=df.columns.drop('Attrition'))\n",
    "    df_test = pd.DataFrame(X[test_idx], columns=df.columns.drop('Attrition'))\n",
    "    \n",
    "    X_train = ss.fit_transform(df_train[feature_columns].values).astype(np.float32)\n",
    "    X_test = ss.fit_transform(df_test[feature_columns].values).astype(np.float32)\n",
    "\n",
    "    y_train = df_train['Attrition_int'].values.astype(np.int)\n",
    "    y_test = df_test['Attrition_int'].values.astype(np.int)\n",
    "\n",
    "    # the whole thing\n",
    "    X_train_num =  df_train[numerics].values\n",
    "    X_test_num = df_test[numerics].values\n",
    "\n",
    "    \n",
    "    # train deep\n",
    "    print('training embedings')\n",
    "    # we need to create separate sequential models for each embedding\n",
    "    embed_branches = []\n",
    "    X_ints_train = []\n",
    "    X_ints_test = []\n",
    "    all_inputs = []\n",
    "    all_branch_outputs = []\n",
    "\n",
    "\n",
    "    # reset this input branch\n",
    "    all_branch_outputs = []\n",
    "    # add in the embeddings\n",
    "    for col in categorical_features:\n",
    "        # encode as ints for the embedding\n",
    "        X_ints_train.append( df_train[col].values )\n",
    "        X_ints_test.append( df_test[col].values )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # also get a dense branch of the numeric features\n",
    "    all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "    x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "    all_branch_outputs.append( x )\n",
    "\n",
    "    # merge the branches together\n",
    "    deep_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "    final_branch = Dense(units=1,activation='sigmoid')(deep_branch)\n",
    "\n",
    "    model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "    model.compile(optimizer='adagrad',\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_ints_train+ [X_train_num],\n",
    "            y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    \n",
    "    # get output from embedding\n",
    "    model2 = Model(inputs=model.inputs, outputs=model.get_layer(index=-2).output)\n",
    "\n",
    "    X_embed = model2.predict(X_ints_train+ [X_train_num])\n",
    "    \n",
    "    # Grid Search Deep\n",
    "    print('Grid Searching the layers of the Deep')\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    num_neurons = [[5, 10], [5, 10, 20]]\n",
    "    class_weight=class_weight = [{0:x, 1:1-x} for x in np.linspace(0.1, 0.5, 2)]\n",
    "    param_grid = dict(num_neurons=num_neurons,\n",
    "                      class_weight=class_weight)\n",
    "\n",
    "    model = KerasClassifier(build_fn=create_model, input_dim=X_embed.shape[-1], epochs=10, verbose=0)\n",
    "    g = GridSearchCV(estimator=model, param_grid=param_grid, verbose=1, scoring='recall')\n",
    "    r = g.fit(X_embed, y_train)\n",
    "    \n",
    "    \n",
    "    # Wide and best deep\n",
    "    \n",
    "    # we need to create separate sequential models for each embedding\n",
    "    embed_branches = []\n",
    "    X_ints_train = []\n",
    "    X_ints_test = []\n",
    "    all_inputs = []\n",
    "    all_branch_outputs = []\n",
    "\n",
    "    for cols in cross_columns:\n",
    "        # encode crossed columns as ints for the embedding\n",
    "        enc = LabelEncoder()\n",
    "\n",
    "        # create crossed labels\n",
    "        # needs to be commented better, Eric!\n",
    "        X_crossed_train = df_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "        X_crossed_test = df_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "        enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "        X_crossed_train = enc.transform(X_crossed_train)\n",
    "        X_crossed_test = enc.transform(X_crossed_test)\n",
    "        X_ints_train.append( X_crossed_train )\n",
    "        X_ints_test.append( X_crossed_test )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # merge the branches together\n",
    "    wide_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "    # reset this input branch\n",
    "    all_branch_outputs = []\n",
    "    # add in the embeddings\n",
    "    for col in categorical_features:\n",
    "        # encode as ints for the embedding\n",
    "        X_ints_train.append( df_train[col].values )\n",
    "        X_ints_test.append( df_test[col].values )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # also get a dense branch of the numeric features\n",
    "    all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "    x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "    all_branch_outputs.append( x )\n",
    "\n",
    "    # merge the branches together\n",
    "    deep_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "    # here is where we'll use the result of the GridSearch\n",
    "    for layer in best_deep.layers[:-1]:\n",
    "        deep_branch = layer(deep_branch)\n",
    "\n",
    "    final_branch = concatenate([wide_branch, deep_branch])\n",
    "    final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "\n",
    "    model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "    model.compile(optimizer='adagrad',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print('fitting the whole thing')\n",
    "    model.fit(X_ints_train+ [X_train_num],\n",
    "            y_train, epochs=10, batch_size=32, verbose=1, class_weight=best_deep_params['class_weight'])\n",
    "    \n",
    "    yhat = np.round(model.predict(X_ints_test + [X_test_num]))\n",
    "    score = mt.recall_score(y_test,yhat)\n",
    "    print(score)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
