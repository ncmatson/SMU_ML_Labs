{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab 6 - wide and deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This data set was created by IBM data scientists.  It describes 35 features for 1470 (fictional) employees including whether or not the employee has left the firm (labeled \"attrition\" in the dataset).  Employees leave companies for a variety of reasons: disatisfaction with their role, their manager or their pay.  Perhaps they aren't necessarily dissatified with their current job but feel like something better is out there.  Or maybe they just feel like they'd been there long enough, and want something different. Most likely its a combination of all of these things, plus a few others.  \n",
    "\n",
    "Employers would like to have a sense of why and when an employee might leave.  If an employer believes that an employee that they really value might leave, they could respond and try to prevent them from leaving.  This is what we will attempt to predict using a wide and deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1102</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>279</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1373</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1392</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>591</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
       "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
       "1   49        No  Travel_Frequently        279  Research & Development   \n",
       "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
       "3   33        No  Travel_Frequently       1392  Research & Development   \n",
       "4   27        No      Travel_Rarely        591  Research & Development   \n",
       "\n",
       "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
       "0                 1          2  Life Sciences              1               1   \n",
       "1                 8          1  Life Sciences              1               2   \n",
       "2                 2          2          Other              1               4   \n",
       "3                 3          4  Life Sciences              1               5   \n",
       "4                 2          1        Medical              1               7   \n",
       "\n",
       "           ...           RelationshipSatisfaction StandardHours  \\\n",
       "0          ...                                  1            80   \n",
       "1          ...                                  4            80   \n",
       "2          ...                                  2            80   \n",
       "3          ...                                  3            80   \n",
       "4          ...                                  4            80   \n",
       "\n",
       "   StockOptionLevel  TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  \\\n",
       "0                 0                  8                      0               1   \n",
       "1                 1                 10                      3               3   \n",
       "2                 0                  7                      3               3   \n",
       "3                 0                  8                      3               3   \n",
       "4                 1                  6                      3               3   \n",
       "\n",
       "   YearsAtCompany YearsInCurrentRole  YearsSinceLastPromotion  \\\n",
       "0               6                  4                        0   \n",
       "1              10                  7                        1   \n",
       "2               0                  0                        0   \n",
       "3               8                  7                        3   \n",
       "4               2                  2                        2   \n",
       "\n",
       "   YearsWithCurrManager  \n",
       "0                     5  \n",
       "1                     7  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../data/'\n",
    "df = pd.read_csv(os.path.join(data_path, 'WA_Fn-UseC_-HR-Employee-Attrition.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakdown of attrition.  (0) Stayed at company, (1) Left company\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "No     1233\n",
       "Yes     237\n",
       "Name: Attrition, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Breakdown of attrition.  (0) Stayed at company, (1) Left company')\n",
    "\n",
    "df.Attrition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expansion\n",
    "\n",
    "This is a relative small dataset with only 1470 data rows. We want more data to train and test. For data expension, we will try several ways to do so. And first we are going to copy and append some rows to origin dataset and expand it to 2000 rows.\n",
    "For another attempt, we take numberical datas from the raw dataframe slice, and add some randomly generat noise to these numerical data. Then we insert categorical data rows back to the dataframe of numerical data with random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_slice = df[:530]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new = df.append(df_slice)\n",
    "df_new = df_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_slice2 = df_slice[['Age','DistanceFromHome','Education','EnvironmentSatisfaction',\n",
    "                      'JobSatisfaction','MonthlyIncome','PerformanceRating','RelationshipSatisfaction',\n",
    "                      'TotalWorkingYears','YearsAtCompany']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use df_slice2 to take numberical datas from the raw dataframe slice, and add some randomly generated noise to these numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_slice2 = df_slice2 * (1 + np.random.uniform(-0.01,0.01,(df_slice2.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice2.insert(1, 'Attrition', df_slice['Attrition'])\n",
    "df_slice2.insert(2, 'Department', df_slice['Department'])\n",
    "df_slice2.insert(6, 'Gender', df_slice['Gender'])\n",
    "df_slice2.insert(8, 'MaritalStatus', df_slice['MaritalStatus'])\n",
    "df_slice2.insert(10, 'OverTime', df_slice['OverTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new2 = df.append(df_slice2)\n",
    "df_new2 = df_new2.reset_index(drop=True)\n",
    "df = df_new2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a dataframe with 2000 rows and 15 clomuns expanded dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 35 features in total in the dataset, but we don't want to use all of them.  \n",
    "Let's focus on a few of them:\n",
    "- Age\n",
    "- Attrition \n",
    "- Department\n",
    "- DistanceFromHome \n",
    "- Education \n",
    "- EduacationField\n",
    "- EnvironmentSatisfaction\n",
    "- Gender\n",
    "- JobSatisfaction\n",
    "- MaritalStatus\n",
    "- MonthlyIncome\n",
    "- OverTime\n",
    "- PerformanceRating\n",
    "- RelationshipSatisfaction\n",
    "- TotalWorkingYears\n",
    "- YearsAtCompany\n",
    "- YearsSinceLastPromotion\n",
    "\n",
    "These features are what we believe important to predict the attrition status. \n",
    "We will use attrition as our label.\n",
    "\n",
    "So let's first drop the other features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 15 columns):\n",
      "Age                         2000 non-null float64\n",
      "Attrition                   2000 non-null object\n",
      "Department                  2000 non-null object\n",
      "DistanceFromHome            2000 non-null float64\n",
      "Education                   2000 non-null float64\n",
      "EnvironmentSatisfaction     2000 non-null float64\n",
      "Gender                      2000 non-null object\n",
      "JobSatisfaction             2000 non-null float64\n",
      "MaritalStatus               2000 non-null object\n",
      "MonthlyIncome               2000 non-null float64\n",
      "OverTime                    2000 non-null object\n",
      "PerformanceRating           2000 non-null float64\n",
      "RelationshipSatisfaction    2000 non-null float64\n",
      "TotalWorkingYears           2000 non-null float64\n",
      "YearsAtCompany              2000 non-null float64\n",
      "dtypes: float64(10), object(5)\n",
      "memory usage: 234.5+ KB\n"
     ]
    }
   ],
   "source": [
    "to_keep = {'Age', 'Attrition', 'Department','DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'Gender', 'JobSatisfaction', 'MaritalStatus',\n",
    "           'MonthlyIncome', 'OverTime', 'PerformanceRating', 'RelationshipSatisfaction','TotalWorkingYears','YearsAtCompany'}\n",
    "to_drop = set(df.columns)-to_keep\n",
    "df.drop(to_drop, axis=1, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakdown of attrition. No = Stayed, Yes = Left\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "No     1680\n",
       "Yes     320\n",
       "Name: Attrition, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Breakdown of attrition. No = Stayed, Yes = Left')\n",
    "\n",
    "df.Attrition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good that we don't have any null value. Let's encode the categorical data to ints. There are some categorical values those have been encoded once from the origin and transfered to type int. We want to use some of them for the cross features, so we want to transfer their type to string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_convert = ['Education','EnvironmentSatisfaction','JobSatisfaction',\n",
    "            'PerformanceRating','RelationshipSatisfaction']\n",
    "\n",
    "for col in to_convert:\n",
    "    df[col] = df[col].astype(np.str) \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "to_encode = { 'Department','Gender','MaritalStatus','OverTime','Education','EnvironmentSatisfaction','JobSatisfaction',\n",
    "            'PerformanceRating','RelationshipSatisfaction'}\n",
    "encoders = dict()\n",
    "\n",
    "for col in list(to_encode) +['Attrition']:\n",
    "    if col=='Attrition':\n",
    "        tmp = LabelEncoder()\n",
    "        df[col] = tmp.fit_transform(df[col])\n",
    "        df[col+'_int'] = tmp.fit_transform(df[col])\n",
    "    else:\n",
    "        encoders[col] = LabelEncoder()\n",
    "        df[col+'_int'] = encoders[col].fit_transform(df[col])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's scale the numeric features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_features =list(to_encode)\n",
    "categorical_features = [x+'_int' for x in categorical_features]\n",
    "numerics = set(df.columns) - to_encode\n",
    "numerics = list(numerics - set(categorical_features)-{'Attrition'})\n",
    "ss = StandardScaler()\n",
    "for atr in numerics:\n",
    "    df[atr] = df[atr].astype(np.float)    \n",
    "    df[atr] = ss.fit_transform(df[atr].values.reshape(-1, 1))\n",
    "    \n",
    "feature_columns = categorical_features + numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 25 columns):\n",
      "Age                             2000 non-null float64\n",
      "Attrition                       2000 non-null int64\n",
      "Department                      2000 non-null object\n",
      "DistanceFromHome                2000 non-null float64\n",
      "Education                       2000 non-null object\n",
      "EnvironmentSatisfaction         2000 non-null object\n",
      "Gender                          2000 non-null object\n",
      "JobSatisfaction                 2000 non-null object\n",
      "MaritalStatus                   2000 non-null object\n",
      "MonthlyIncome                   2000 non-null float64\n",
      "OverTime                        2000 non-null object\n",
      "PerformanceRating               2000 non-null object\n",
      "RelationshipSatisfaction        2000 non-null object\n",
      "TotalWorkingYears               2000 non-null float64\n",
      "YearsAtCompany                  2000 non-null float64\n",
      "MaritalStatus_int               2000 non-null int64\n",
      "EnvironmentSatisfaction_int     2000 non-null int64\n",
      "OverTime_int                    2000 non-null int64\n",
      "Department_int                  2000 non-null int64\n",
      "Education_int                   2000 non-null int64\n",
      "JobSatisfaction_int             2000 non-null int64\n",
      "RelationshipSatisfaction_int    2000 non-null int64\n",
      "Gender_int                      2000 non-null int64\n",
      "PerformanceRating_int           2000 non-null int64\n",
      "Attrition_int                   2000 non-null float64\n",
      "dtypes: float64(6), int64(10), object(9)\n",
      "memory usage: 390.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric\n",
    "\n",
    "Let's take a moment to break down what the three main metrics mean in our task, i.e. predicting whether or not an employee will leave a company, and what that mean for our businesses using the model:\n",
    "\n",
    "* accuracy: Values getting true positives and negatives.  This probably isn't very useful for our dataset because there are significantly fewer people that left the company than stayed\n",
    "\n",
    "* precision: Values a low false positive rate.  This probably isn't the best either, because while we wouldn't **want** to think that an employee is leaving when they aren't, it probably won't hurt the business, unless the employer grossly overreacts and scares them away\n",
    "\n",
    "* recall: Values a low false negative rate.  This is the best metric for our case.  If our job is to see when employees leave, and if the fact is that they usually **don't** leave, and if its potentially pretty damaging to the firm when the employee **does** leave, we want to make sure that we miss as few cases as possible.\n",
    "\n",
    "\n",
    "### Validation Method\n",
    "\n",
    "Because of the imbalance in our prediction label we'll use a stratified split, this way we'll preserve the distribution in our model.  In an attempt to realistically generalize the overall performance of our model we'll use a nested cross-validation scheme.  We'll use k-fold as opposed to a different cv scheme like shuffle-split, because our dataset is not that large and we would like to train on as much data as possible.  Using a k-fold cv ensures that we train on all of our data.  The inner loop will tune the hyper-parameters of our model which will be discussed later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "Let's just see how we well we can do with a singl network on all of our data.  We'll just split all of our data up into one train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (1800, 15) test (200, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# stratified 90/10 train/test split`\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, stratify=df.Attrition)\n",
    "\n",
    "X_train =  ss.fit_transform(df_train[feature_columns].values).astype(np.float32)\n",
    "X_test =  ss.transform(df_test[feature_columns].values).astype(np.float32)\n",
    "\n",
    "y_train = df_train['Attrition'].values.astype(np.int)\n",
    "y_test = df_test['Attrition'].values.astype(np.int)\n",
    "\n",
    "print('train', X_train.shape, 'test', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import some keras stuff\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.layers import Embedding, Flatten, Merge, concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This returns a tensor\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(units=10, activation='relu')(inputs)\n",
    "predictions = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dense_168 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 171\n",
      "Trainable params: 171\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[158  10]\n",
      " [ 17  15]] 0.46875\n",
      "Wall time: 2.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=50, verbose=0)\n",
    "\n",
    "from sklearn import metrics as mt\n",
    "yhat = np.round(model.predict(X_test))\n",
    "print(mt.confusion_matrix(y_test,yhat),mt.recall_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that didn't do very well, but there are a few issues\n",
    "\n",
    "1. We have a pretty big class imbalance and not a lot of positives, which makes it difficult to evaluate the loss function\n",
    "\n",
    "2. We were using a mean squared error for our loss function on a binary classification task.  While this isn't terrible, it would likely work better if we used cross-entropy instead.\n",
    "\n",
    "Luckily its easy to modify both of these in Keras.  The loss function is simple to change, and the fit function includes a parameter for 'class_weights' which accepts a dictionary of weights for each class value to use when computing the loss function.  This way we can tell the model which class is \"more important\".  Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[154  14]\n",
      " [  4  28]] 0.875\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=50, verbose=0, class_weight={0 : 0.20, 1 : 0.80})\n",
    "\n",
    "yhat = np.round(model.predict(X_test))\n",
    "print(mt.confusion_matrix(y_test,yhat),mt.recall_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, looks like we might want to tune that parameter of class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning in Deep Network\n",
    "\n",
    "We identify the following as parameters that we can tune\n",
    "\n",
    "- Number of layers\n",
    "- Number of neurons per layer\n",
    "- The weights of the class in the loss function\n",
    "- number of epochs\n",
    "\n",
    "Keras has a wrapper class for a model to be used as an sklearn estimator which we can that pass to GridSearchCV.  The only caveat is that we must use Keras' Sequential Model to do so which means we can only have one input branch.  This is okay though because if we only build it on the deep side we can use the result of that with our wide, cross-category branch later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "# this uses the Keras Wrapper to make the model usable by sk-learn\n",
    "\n",
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(num_neurons=12, input_dim=8):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # num_neurons is a list of the number nuerons at each layer\n",
    "    for layer, num in enumerate(num_neurons):\n",
    "        if layer == 0:\n",
    "            model.add(Dense(num, input_dim=input_dim, activation='relu'))\n",
    "        else:\n",
    "            model.add(Dense(units=num, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can creat models in that can be used by SKLearn, lets GridSearch over the parameters we identified.  Note that the number of neurons and number of layers is combined into one parameter called `num_neurons` which is a list of the number of output nerouns at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, epochs=10, num_neurons=[5, 10] \n",
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, epochs=10, num_neurons=[5, 10], score=1.0, total=   2.6s\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, epochs=10, num_neurons=[5, 10] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, epochs=10, num_neurons=[5, 10], score=1.0, total=   2.7s\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, epochs=10, num_neurons=[5, 10] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    5.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, epochs=10, num_neurons=[5, 10], score=0.979381443298969, total=   3.5s\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, epochs=10, num_neurons=[5, 10, 20] \n",
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, epochs=10, num_neurons=[5, 10, 20], score=0.9893617021276596, total=   3.7s\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, epochs=10, num_neurons=[5, 10, 20] \n",
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, epochs=10, num_neurons=[5, 10, 20], score=1.0, total=   3.0s\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, epochs=10, num_neurons=[5, 10, 20] \n",
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, epochs=10, num_neurons=[5, 10, 20], score=0.9381443298969072, total=   2.8s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, epochs=10, num_neurons=[5, 10] ...\n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, epochs=10, num_neurons=[5, 10], score=0.0, total=   3.8s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, epochs=10, num_neurons=[5, 10] ...\n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, epochs=10, num_neurons=[5, 10], score=0.8865979381443299, total=   3.2s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, epochs=10, num_neurons=[5, 10] ...\n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, epochs=10, num_neurons=[5, 10], score=0.0, total=   3.2s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, epochs=10, num_neurons=[5, 10, 20] \n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, epochs=10, num_neurons=[5, 10, 20], score=0.7446808510638298, total=   5.0s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, epochs=10, num_neurons=[5, 10, 20] \n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, epochs=10, num_neurons=[5, 10, 20], score=0.9484536082474226, total=   3.1s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, epochs=10, num_neurons=[5, 10, 20] \n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, epochs=10, num_neurons=[5, 10, 20], score=0.9896907216494846, total=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:   41.1s finished\n"
     ]
    }
   ],
   "source": [
    "# this will move inside nested CV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "num_neurons = [[5, 10], [5, 10, 20]]\n",
    "epochs = [10]\n",
    "class_weight = [{0:x, 1:1-x} for x in np.linspace(0.1, 0.5, 2)]\n",
    "param_grid = dict(num_neurons=num_neurons,\n",
    "                  epochs=epochs,\n",
    "                  class_weight=class_weight)\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, input_dim=X_train.shape[-1], epochs=10, verbose=0)\n",
    "g = GridSearchCV(estimator=model, param_grid=param_grid, verbose=3, scoring='recall')\n",
    "r = g.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_epochs</th>\n",
       "      <th>param_num_neurons</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.257783</td>\n",
       "      <td>0.788941</td>\n",
       "      <td>0.993127</td>\n",
       "      <td>0.968640</td>\n",
       "      <td>{0: 0.1, 1: 0.9}</td>\n",
       "      <td>10</td>\n",
       "      <td>[5, 10]</td>\n",
       "      <td>{'class_weight': {0: 0.1, 1: 0.9}, 'epochs': 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958115</td>\n",
       "      <td>0.979381</td>\n",
       "      <td>0.958115</td>\n",
       "      <td>0.216405</td>\n",
       "      <td>0.160885</td>\n",
       "      <td>0.009720</td>\n",
       "      <td>0.014885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.491110</td>\n",
       "      <td>0.774675</td>\n",
       "      <td>0.975835</td>\n",
       "      <td>0.986065</td>\n",
       "      <td>{0: 0.1, 1: 0.9}</td>\n",
       "      <td>10</td>\n",
       "      <td>[5, 10, 20]</td>\n",
       "      <td>{'class_weight': {0: 0.1, 1: 0.9}, 'epochs': 1...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.989362</td>\n",
       "      <td>0.994845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938144</td>\n",
       "      <td>0.963351</td>\n",
       "      <td>0.319765</td>\n",
       "      <td>0.084749</td>\n",
       "      <td>0.027003</td>\n",
       "      <td>0.016199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.568314</td>\n",
       "      <td>0.920839</td>\n",
       "      <td>0.295533</td>\n",
       "      <td>0.307155</td>\n",
       "      <td>{0: 0.5, 1: 0.5}</td>\n",
       "      <td>10</td>\n",
       "      <td>[5, 10]</td>\n",
       "      <td>{'class_weight': {0: 0.5, 1: 0.5}, 'epochs': 1...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>0.921466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163672</td>\n",
       "      <td>0.123668</td>\n",
       "      <td>0.417946</td>\n",
       "      <td>0.434383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.917350</td>\n",
       "      <td>0.859220</td>\n",
       "      <td>0.894275</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>{0: 0.5, 1: 0.5}</td>\n",
       "      <td>10</td>\n",
       "      <td>[5, 10, 20]</td>\n",
       "      <td>{'class_weight': {0: 0.5, 1: 0.5}, 'epochs': 1...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.948454</td>\n",
       "      <td>0.963351</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886745</td>\n",
       "      <td>0.070186</td>\n",
       "      <td>0.107110</td>\n",
       "      <td>0.123487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       2.257783         0.788941         0.993127          0.968640   \n",
       "1       2.491110         0.774675         0.975835          0.986065   \n",
       "2       2.568314         0.920839         0.295533          0.307155   \n",
       "3       2.917350         0.859220         0.894275          0.895000   \n",
       "\n",
       "  param_class_weight param_epochs param_num_neurons  \\\n",
       "0   {0: 0.1, 1: 0.9}           10           [5, 10]   \n",
       "1   {0: 0.1, 1: 0.9}           10       [5, 10, 20]   \n",
       "2   {0: 0.5, 1: 0.5}           10           [5, 10]   \n",
       "3   {0: 0.5, 1: 0.5}           10       [5, 10, 20]   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "0  {'class_weight': {0: 0.1, 1: 0.9}, 'epochs': 1...                1   \n",
       "1  {'class_weight': {0: 0.1, 1: 0.9}, 'epochs': 1...                2   \n",
       "2  {'class_weight': {0: 0.5, 1: 0.5}, 'epochs': 1...                4   \n",
       "3  {'class_weight': {0: 0.5, 1: 0.5}, 'epochs': 1...                3   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           1.000000            0.989691           1.000000   \n",
       "1           0.989362            0.994845           1.000000   \n",
       "2           0.000000            0.000000           0.886598   \n",
       "3           0.744681            0.721649           0.948454   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0            0.958115           0.979381            0.958115      0.216405   \n",
       "1            1.000000           0.938144            0.963351      0.319765   \n",
       "2            0.921466           0.000000            0.000000      0.163672   \n",
       "3            0.963351           0.989691            1.000000      0.886745   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.160885        0.009720         0.014885  \n",
       "1        0.084749        0.027003         0.016199  \n",
       "2        0.123668        0.417946         0.434383  \n",
       "3        0.070186        0.107110         0.123487  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(r.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did a lot better with the more divergent class weights and the greater number of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide and Deep\n",
    "\n",
    "Now let's combine a sparse branch with cross categories and a deep branch where we perform a grid search on the number of layers.\n",
    "\n",
    "Let's pick some categories to cross that might contain some interesting information about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_columns = [['Gender','MaritalStatus'],\n",
    "                    ['Education', 'JobSatisfaction'],['Department','PerformanceRating'],\n",
    "                    ['Education', 'JobSatisfaction','RelationshipSatisfaction'],['Department','OverTime'],\n",
    "                ]\n",
    "\n",
    "X_train_num =  df_train[numerics].values\n",
    "X_test_num = df_test[numerics].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train the embeddings, and then take that result and run it through our GridSearched Deep Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14e01091e80>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to create separate sequential models for each embedding\n",
    "embed_branches = []\n",
    "X_ints_train = []\n",
    "X_ints_test = []\n",
    "all_inputs = []\n",
    "all_branch_outputs = []\n",
    "\n",
    "\n",
    "# reset this input branch\n",
    "all_branch_outputs = []\n",
    "# add in the embeddings\n",
    "for col in categorical_features:\n",
    "    # encode as ints for the embedding\n",
    "    X_ints_train.append( df_train[col].values )\n",
    "    X_ints_test.append( df_test[col].values )\n",
    "    \n",
    "    # get the number of categories\n",
    "    N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "    \n",
    "    # create embedding branch from the number of categories\n",
    "    inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "    all_inputs.append(inputs)\n",
    "    x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "    x = Flatten()(x)\n",
    "    all_branch_outputs.append(x)\n",
    "    \n",
    "# also get a dense branch of the numeric features\n",
    "all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "all_branch_outputs.append( x )\n",
    "\n",
    "# merge the branches together\n",
    "deep_branch = concatenate(all_branch_outputs)\n",
    "    \n",
    "final_branch = Dense(units=1,activation='sigmoid')(deep_branch)\n",
    "\n",
    "model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "model.compile(optimizer='adagrad',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_ints_train+ [X_train_num],\n",
    "        y_train, epochs=10, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grab the results right before the output layer, this is the realization of all the embedded data\n",
    "\n",
    "model2 = Model(inputs=model.inputs, outputs=model.get_layer(index=-2).output)\n",
    "\n",
    "X_embed = model2.predict(X_ints_train+ [X_train_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put that through the gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10] \n",
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10], score=1.0, total=   3.2s\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10], score=1.0, total=   4.5s\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    7.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10], score=1.0, total=   3.4s\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10, 20] \n",
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10, 20], score=1.0, total=   3.3s\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10, 20] \n",
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10, 20], score=1.0, total=   3.9s\n",
      "[CV] class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10, 20] \n",
      "[CV]  class_weight={0: 0.10000000000000001, 1: 0.90000000000000002}, num_neurons=[5, 10, 20], score=1.0, total=   4.4s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10] ..............\n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10], score=0.9680851063829787, total=   4.7s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10] ..............\n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10], score=1.0, total=   3.7s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10] ..............\n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10], score=1.0, total=   4.7s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10, 20] ..........\n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10, 20], score=1.0, total=   5.7s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10, 20] ..........\n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10, 20], score=1.0, total=   4.8s\n",
      "[CV] class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10, 20] ..........\n",
      "[CV]  class_weight={0: 0.5, 1: 0.5}, num_neurons=[5, 10, 20], score=1.0, total=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:   52.8s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "num_neurons = [[5, 10], [5, 10, 20]]\n",
    "class_weight=class_weight = [{0:x, 1:1-x} for x in np.linspace(0.1, 0.5, 2)]\n",
    "param_grid = dict(num_neurons=num_neurons,\n",
    "                  class_weight=class_weight)\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, input_dim=X_embed.shape[-1], epochs=10, verbose=0)\n",
    "g = GridSearchCV(estimator=model, param_grid=param_grid, verbose=3, scoring='recall')\n",
    "r = g.fit(X_embed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the best model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_258 (Dense)            (None, 5)                 700       \n",
      "_________________________________________________________________\n",
      "dense_259 (Dense)            (None, 10)                60        \n",
      "_________________________________________________________________\n",
      "dense_260 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 771\n",
      "Trainable params: 771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "{'input_dim': 139, 'epochs': 10, 'verbose': 0, 'class_weight': {0: 0.10000000000000001, 1: 0.90000000000000002}, 'num_neurons': [5, 10], 'build_fn': <function create_model at 0x0000014E08D30730>}\n"
     ]
    }
   ],
   "source": [
    "best_deep = r.best_estimator_.model\n",
    "print(best_deep.summary())\n",
    "best_deep_params = r.best_estimator_.get_params()\n",
    "print(best_deep_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this model in our combined wide and deep network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1800/1800 [==============================] - 1s - loss: 0.0913 - acc: 0.7072     \n",
      "Epoch 2/10\n",
      "1800/1800 [==============================] - 0s - loss: 0.0298 - acc: 0.9822     \n",
      "Epoch 3/10\n",
      "1800/1800 [==============================] - 0s - loss: 0.0118 - acc: 1.0000     \n",
      "Epoch 4/10\n",
      "1800/1800 [==============================] - 0s - loss: 0.0060 - acc: 1.0000     \n",
      "Epoch 5/10\n",
      "1800/1800 [==============================] - 0s - loss: 0.0036 - acc: 1.0000     \n",
      "Epoch 6/10\n",
      "1800/1800 [==============================] - 0s - loss: 0.0025 - acc: 1.0000     \n",
      "Epoch 7/10\n",
      "1800/1800 [==============================] - 0s - loss: 0.0018 - acc: 1.0000     \n",
      "Epoch 8/10\n",
      "1800/1800 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 9/10\n",
      "1800/1800 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 10/10\n",
      "1800/1800 [==============================] - 0s - loss: 9.3714e-04 - acc: 1.0000     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14e18d37f60>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to create separate sequential models for each embedding\n",
    "embed_branches = []\n",
    "X_ints_train = []\n",
    "X_ints_test = []\n",
    "all_inputs = []\n",
    "all_branch_outputs = []\n",
    "\n",
    "for cols in cross_columns:\n",
    "    # encode crossed columns as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "    \n",
    "    # create crossed labels\n",
    "    # needs to be commented better, Eric!\n",
    "    X_crossed_train = df_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "    X_crossed_test = df_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "    \n",
    "    enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "    X_crossed_train = enc.transform(X_crossed_train)\n",
    "    X_crossed_test = enc.transform(X_crossed_test)\n",
    "    X_ints_train.append( X_crossed_train )\n",
    "    X_ints_test.append( X_crossed_test )\n",
    "    \n",
    "    # get the number of categories\n",
    "    N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "    \n",
    "    # create embedding branch from the number of categories\n",
    "    inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "    all_inputs.append(inputs)\n",
    "    x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "    x = Flatten()(x)\n",
    "    all_branch_outputs.append(x)\n",
    "    \n",
    "# merge the branches together\n",
    "wide_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "# reset this input branch\n",
    "all_branch_outputs = []\n",
    "# add in the embeddings\n",
    "for col in categorical_features:\n",
    "    # encode as ints for the embedding\n",
    "    X_ints_train.append( df_train[col].values )\n",
    "    X_ints_test.append( df_test[col].values )\n",
    "    \n",
    "    # get the number of categories\n",
    "    N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "    \n",
    "    # create embedding branch from the number of categories\n",
    "    inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "    all_inputs.append(inputs)\n",
    "    x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "    x = Flatten()(x)\n",
    "    all_branch_outputs.append(x)\n",
    "    \n",
    "# also get a dense branch of the numeric features\n",
    "all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "all_branch_outputs.append( x )\n",
    "\n",
    "# merge the branches together\n",
    "deep_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "# here is where we'll use the result of the GridSearch\n",
    "for layer in best_deep.layers[:-1]:\n",
    "    deep_branch = layer(deep_branch)\n",
    "    \n",
    "final_branch = concatenate([wide_branch, deep_branch])\n",
    "final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "\n",
    "model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "model.compile(optimizer='adagrad',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_ints_train+ [X_train_num],\n",
    "        y_train, epochs=10, batch_size=32, verbose=1, class_weight=best_deep_params['class_weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, check out how it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[168   0]\n",
      " [  0  32]] 1.0\n"
     ]
    }
   ],
   "source": [
    "yhat = np.round(model.predict(X_ints_test + [X_test_num]))\n",
    "print(mt.confusion_matrix(y_test,yhat),mt.recall_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well it got all of them right....  Let's see if we can generalize that using a nested grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization using nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************************************\n",
      "training embedings\n",
      "************************************\n",
      "\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 2s - loss: 0.1222 - acc: 0.8962     \n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0591 - acc: 0.9756     \n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0367 - acc: 0.9912     \n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0251 - acc: 0.9925     \n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0184 - acc: 0.9938     \n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0140 - acc: 0.9963     \n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0111 - acc: 0.9975     \n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0090 - acc: 0.9981     \n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0075 - acc: 0.9981     \n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0063 - acc: 0.9981     \n",
      "\n",
      "************************************\n",
      "Grid Searching the layers of the Deep\n",
      "************************************\n",
      "\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************************************\n",
      "fitting the whole thing\n",
      "*************************************\n",
      "\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 3s - loss: 0.0796 - acc: 0.7600     \n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0165 - acc: 0.9963     \n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0063 - acc: 0.9994     \n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0034 - acc: 0.9994     \n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0022 - acc: 0.9994     \n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0015 - acc: 0.9994     \n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 0s - loss: 8.8282e-04 - acc: 1.0000     \n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 0s - loss: 7.1550e-04 - acc: 1.0000     \n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 0s - loss: 5.9444e-04 - acc: 1.0000     \n",
      "\n",
      "*************************************\n",
      "Overall Accuracy loop 1 : 1.0\n",
      "*************************************\n",
      "\n",
      "\n",
      "************************************\n",
      "training embedings\n",
      "************************************\n",
      "\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 2s - loss: 0.1399 - acc: 0.8656     \n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0782 - acc: 0.9381     \n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0502 - acc: 0.9788     \n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0342 - acc: 0.9888     \n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0245 - acc: 0.9931     \n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0184 - acc: 0.9950     \n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0143 - acc: 0.9963     \n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0115 - acc: 0.9981     \n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0094 - acc: 0.9981     \n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0079 - acc: 0.9988     \n",
      "\n",
      "************************************\n",
      "Grid Searching the layers of the Deep\n",
      "************************************\n",
      "\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************************************\n",
      "fitting the whole thing\n",
      "*************************************\n",
      "\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 3s - loss: 0.1469 - acc: 0.8937     \n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0856 - acc: 0.9963     \n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0566 - acc: 0.9994     \n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0403 - acc: 1.0000     \n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0303 - acc: 1.0000     \n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0237 - acc: 1.0000     \n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0191 - acc: 1.0000     \n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0158 - acc: 1.0000     \n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0133 - acc: 1.0000     \n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0114 - acc: 1.0000     \n",
      "\n",
      "*************************************\n",
      "Overall Accuracy loop 2 : 1.0\n",
      "*************************************\n",
      "\n",
      "\n",
      "************************************\n",
      "training embedings\n",
      "************************************\n",
      "\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 3s - loss: 0.1551 - acc: 0.8363     \n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0955 - acc: 0.8912     \n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0666 - acc: 0.9450     \n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0482 - acc: 0.9694     \n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0362 - acc: 0.9788     \n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0283 - acc: 0.9838     \n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0229 - acc: 0.9888     \n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0190 - acc: 0.9906     \n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0162 - acc: 0.9919     \n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0139 - acc: 0.9931     \n",
      "\n",
      "************************************\n",
      "Grid Searching the layers of the Deep\n",
      "************************************\n",
      "\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************************************\n",
      "fitting the whole thing\n",
      "*************************************\n",
      "\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 4s - loss: 0.0552 - acc: 0.9525     \n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0083 - acc: 0.9956     \n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0032 - acc: 0.9988     \n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0018 - acc: 1.0000     \n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 0s - loss: 8.5130e-04 - acc: 1.0000     \n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 0s - loss: 6.5561e-04 - acc: 1.0000     \n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 0s - loss: 5.2660e-04 - acc: 1.0000     \n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 0s - loss: 4.3637e-04 - acc: 1.0000     \n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 0s - loss: 3.7021e-04 - acc: 1.0000     \n",
      "\n",
      "*************************************\n",
      "Overall Accuracy loop 3 : 1.0\n",
      "*************************************\n",
      "\n",
      "\n",
      "************************************\n",
      "training embedings\n",
      "************************************\n",
      "\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 4s - loss: 0.1301 - acc: 0.8600     \n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0732 - acc: 0.9219     \n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0468 - acc: 0.9762     \n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0323 - acc: 0.9869     \n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0238 - acc: 0.9906     \n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0184 - acc: 0.9919     \n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0146 - acc: 0.9950     \n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0119 - acc: 0.9950     \n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0099 - acc: 0.9969     \n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0083 - acc: 0.9969     \n",
      "\n",
      "************************************\n",
      "Grid Searching the layers of the Deep\n",
      "************************************\n",
      "\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************************************\n",
      "fitting the whole thing\n",
      "*************************************\n",
      "\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 5s - loss: 0.0976 - acc: 0.6038     \n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0191 - acc: 0.9919     \n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0051 - acc: 0.9981     \n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0025 - acc: 1.0000     \n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0015 - acc: 1.0000     \n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 0s - loss: 8.0446e-04 - acc: 1.0000     \n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 0s - loss: 6.3273e-04 - acc: 1.0000     \n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 0s - loss: 5.1717e-04 - acc: 1.0000     \n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 0s - loss: 4.3183e-04 - acc: 1.0000     \n",
      "\n",
      "*************************************\n",
      "Overall Accuracy loop 4 : 1.0\n",
      "*************************************\n",
      "\n",
      "\n",
      "************************************\n",
      "training embedings\n",
      "************************************\n",
      "\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 4s - loss: 0.1110 - acc: 0.9069     \n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0496 - acc: 0.9825     \n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0324 - acc: 0.9912     \n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0230 - acc: 0.9963     \n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0171 - acc: 0.9975     \n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0132 - acc: 0.9988     \n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0105 - acc: 1.0000     \n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0086 - acc: 1.0000     \n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0072 - acc: 1.0000     \n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0061 - acc: 1.0000     \n",
      "\n",
      "************************************\n",
      "Grid Searching the layers of the Deep\n",
      "************************************\n",
      "\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************************************\n",
      "fitting the whole thing\n",
      "*************************************\n",
      "\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 6s - loss: 0.1040 - acc: 0.8800     \n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0302 - acc: 0.9994     \n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0119 - acc: 1.0000     \n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0063 - acc: 1.0000     - ETA: 0s - loss: 0.0076 - a\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.0041 - acc: 1.000 - 0s - loss: 0.0040 - acc: 1.0000     \n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0029 - acc: 1.0000     \n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0022 - acc: 1.0000     \n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0017 - acc: 1.0000     \n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[15,0] = 532 is not in [0, 531)\n\t [[Node: embedding_241/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_241/embeddings/read, _arg_RelationshipSatisfaction_int_21_0_13)]]\n\nCaused by op 'embedding_241/Gather', defined at:\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-134-d4d94ba85818>\", line 154, in <module>\n    x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\engine\\topology.py\", line 602, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 134, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1134, in gather\n    return tf.gather(reference, indices)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1179, in gather\n    validate_indices=validate_indices, name=name)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[15,0] = 532 is not in [0, 531)\n\t [[Node: embedding_241/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_241/embeddings/read, _arg_RelationshipSatisfaction_int_21_0_13)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[15,0] = 532 is not in [0, 531)\n\t [[Node: embedding_241/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_241/embeddings/read, _arg_RelationshipSatisfaction_int_21_0_13)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-d4d94ba85818>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    184\u001b[0m             y_train, epochs=10, batch_size=32, verbose=1, class_weight=best_deep_params['class_weight'])\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_ints_test\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX_test_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n*************************************'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1711\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1712\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[1;32m-> 1713\u001b[1;33m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[0;32m   1714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1267\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1269\u001b[1;33m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1270\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[15,0] = 532 is not in [0, 531)\n\t [[Node: embedding_241/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_241/embeddings/read, _arg_RelationshipSatisfaction_int_21_0_13)]]\n\nCaused by op 'embedding_241/Gather', defined at:\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-134-d4d94ba85818>\", line 154, in <module>\n    x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\engine\\topology.py\", line 602, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 134, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1134, in gather\n    return tf.gather(reference, indices)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1179, in gather\n    validate_indices=validate_indices, name=name)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Cameron\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[15,0] = 532 is not in [0, 531)\n\t [[Node: embedding_241/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_241/embeddings/read, _arg_RelationshipSatisfaction_int_21_0_13)]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "outer_loop = StratifiedKFold(n_splits=5)\n",
    "\n",
    "X = df.drop('Attrition', axis=1).values\n",
    "y = df.Attrition.values\n",
    "\n",
    "scores = []\n",
    "i = 1\n",
    "\n",
    "for train_idx, test_idx in outer_loop.split(X, y):\n",
    "    \n",
    "    # split data\n",
    "    df_train = pd.DataFrame(X[train_idx], columns=df.columns.drop('Attrition'))\n",
    "    df_test = pd.DataFrame(X[test_idx], columns=df.columns.drop('Attrition'))\n",
    "    \n",
    "    X_train = ss.fit_transform(df_train[feature_columns].values).astype(np.float32)\n",
    "    X_test = ss.fit_transform(df_test[feature_columns].values).astype(np.float32)\n",
    "\n",
    "    y_train = y[train_idx].astype(np.int)\n",
    "    y_test = y[test_idx].astype(np.int)\n",
    "\n",
    "    # the whole thing\n",
    "    X_train_num =  df_train[numerics].values\n",
    "    X_test_num = df_test[numerics].values\n",
    "\n",
    "    \n",
    "    # train deep\n",
    "    print('\\n************************************')    \n",
    "    print('training embedings')\n",
    "    print('************************************\\n')\n",
    "\n",
    "    # we need to create separate sequential models for each embedding\n",
    "    embed_branches = []\n",
    "    X_ints_train = []\n",
    "    X_ints_test = []\n",
    "    all_inputs = []\n",
    "    all_branch_outputs = []\n",
    "\n",
    "\n",
    "    # reset this input branch\n",
    "    all_branch_outputs = []\n",
    "    # add in the embeddings\n",
    "    for col in categorical_features:\n",
    "        # encode as ints for the embedding\n",
    "        X_ints_train.append( df_train[col].values )\n",
    "        X_ints_test.append( df_test[col].values )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # also get a dense branch of the numeric features\n",
    "    all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "    x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "    all_branch_outputs.append( x )\n",
    "\n",
    "    # merge the branches together\n",
    "    deep_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "    final_branch = Dense(units=1,activation='sigmoid')(deep_branch)\n",
    "\n",
    "    model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "    model.compile(optimizer='adagrad',\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_ints_train+ [X_train_num],\n",
    "            y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    \n",
    "    # get output from embedding\n",
    "    model2 = Model(inputs=model.inputs, outputs=model.get_layer(index=-2).output)\n",
    "\n",
    "    X_embed = model2.predict(X_ints_train+ [X_train_num])\n",
    "    \n",
    "    # Grid Search Deep\n",
    "    print('\\n************************************')\n",
    "    print('Grid Searching the layers of the Deep')\n",
    "    print('************************************\\n')\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    num_neurons = [[5, 10], [5, 10, 20], [15, 15, 10]]\n",
    "    class_weight=class_weight = [{0:x, 1:1-x} for x in np.linspace(0.1, 0.5, 3)]\n",
    "    param_grid = dict(num_neurons=num_neurons,\n",
    "                      class_weight=class_weight)\n",
    "\n",
    "    model = KerasClassifier(build_fn=create_model, input_dim=X_embed.shape[-1], epochs=10, verbose=0)\n",
    "    g = GridSearchCV(estimator=model, param_grid=param_grid, verbose=1, scoring='recall')\n",
    "    r = g.fit(X_embed, y_train)\n",
    "    \n",
    "    best_deep = r.best_estimator_.model\n",
    "    best_deep_params = r.best_estimator_.get_params()\n",
    "    \n",
    "    \n",
    "    # Wide and best deep\n",
    "    \n",
    "    # we need to create separate sequential models for each embedding\n",
    "    embed_branches = []\n",
    "    X_ints_train = []\n",
    "    X_ints_test = []\n",
    "    all_inputs = []\n",
    "    all_branch_outputs = []\n",
    "\n",
    "    for cols in cross_columns:\n",
    "        # encode crossed columns as ints for the embedding\n",
    "        enc = LabelEncoder()\n",
    "\n",
    "        # create crossed labels\n",
    "        # needs to be commented better, Eric!\n",
    "        X_crossed_train = df_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "        X_crossed_test = df_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "        enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "        X_crossed_train = enc.transform(X_crossed_train)\n",
    "        X_crossed_test = enc.transform(X_crossed_test)\n",
    "        X_ints_train.append( X_crossed_train )\n",
    "        X_ints_test.append( X_crossed_test )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # merge the branches together\n",
    "    wide_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "    # reset this input branch\n",
    "    all_branch_outputs = []\n",
    "    # add in the embeddings\n",
    "    for col in categorical_features:\n",
    "        # encode as ints for the embedding\n",
    "        X_ints_train.append( df_train[col].values )\n",
    "        X_ints_test.append( df_test[col].values )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # also get a dense branch of the numeric features\n",
    "    all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "    x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "    all_branch_outputs.append( x )\n",
    "\n",
    "    # merge the branches together\n",
    "    deep_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "    # here is where we'll use the result of the GridSearch\n",
    "    for layer in best_deep.layers[:-1]:\n",
    "        deep_branch = layer(deep_branch)\n",
    "\n",
    "    final_branch = concatenate([wide_branch, deep_branch])\n",
    "    final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "\n",
    "    model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "    model.compile(optimizer='adagrad',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print('\\n************************************')\n",
    "    print('fitting the whole thing')\n",
    "    print('*************************************\\n')\n",
    "\n",
    "    model.fit(X_ints_train+ [X_train_num],\n",
    "            y_train, epochs=10, batch_size=32, verbose=1, class_weight=best_deep_params['class_weight'])\n",
    "    \n",
    "    yhat = np.round(model.predict(X_ints_test + [X_test_num]))\n",
    "    score = mt.recall_score(y_test,yhat)\n",
    "    print('\\n*************************************')\n",
    "    print('Overall Accuracy loop', i, \":\", score)\n",
    "    print('*************************************\\n')\n",
    "    scores.append(score)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't what that error is^ but it finished everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean performance: 1.0 (+- 0.0 )\n"
     ]
    }
   ],
   "source": [
    "print('Mean performance:', np.mean(scores), \"(+-\", np.std(scores), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looked like it did pretty well.  Really so obviously this isn't a very difficult probelm, even though we expanded the dataset with some random data, although this probably did help to regularize the data.\n",
    "\n",
    "When compared to the plain MLP that we ran at the beginning of the notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
